# Chapter 5. 트리 알고리즘

## 5-1. 결정 트리 (Decision Tree)

### 1) 로지스틱 회귀로 와인 분류

- `DataFrame.info()`: 데이터프레임의 행, 열, 결측치 여부 등 메타 정보 확인
- `DataFrame.describe()`: 기초 통계 요약 정보 제공
- 주요 통계 지표:
  - **mean**: 평균
  - **std**: 표준편차
  - **min / max**: 최소 / 최대값
  - **25% / 50% / 75%**: 1사분위수, 중간값(2사분위수), 3사분위수

### 2) 결정 트리란?

- **스무고개 게임**처럼 질문을 던져가며 정답을 좁혀가는 방식
- 데이터를 잘 나눌 수 있는 질문을 찾아 **분류 정확도**를 높이는 것이 목표
- 사이킷런 클래스: `DecisionTreeClassifier`

#### 📌 결정 트리 구조 용어

| 용어         | 설명 |
|------------|------|
| 루트 노드    | 트리의 맨 위 노드, 처음 분할되는 기준이 되는 노드 |
| 내부 노드    | 중간에 위치하여 분기를 만드는 노드 |
| 리프 노드    | 더 이상 분할되지 않는 최종 노드, 예측 결과를 나타냄 |

- 시각화 시 `figsize()`를 통해 그림 크기(단위: 인치)를 조정
- 리프 노드를 제한 없이 늘리면 **과대적합(Overfitting)** 발생
- 따라서 **언제까지 이분화를 할지**에 대한 조건 필요 → **가지치기(pruning)**로 제어

#### ✅ 노드 설명 요소

- **test condition**: 노드가 샘플을 분리하는 기준
- **samples**: 현재 노드에 속한 샘플 수
- **value**: 각 클래스별 샘플 수 (예: [양성, 음성])
- 조건 만족 시 왼쪽 자식 노드, 그렇지 않으면 오른쪽 자식 노드로 이동

#### 🌈 시각적 특징

- 클래스 비율이 뚜렷할수록 색이 진하게 표시됨
- 리프 노드에서는 **가장 많은 클래스**가 예측 클래스가 됨

#### 🔎 불순도 개념

- **지니 불순도(Gini Impurity)**:
  
  $$
  \text{Gini} = 1 - \sum_{k=1}^{K} (p_k)^2
  $$

  예: 이진 분류일 경우,  
  $$
  \text{Gini} = 1 - (\text{양성 비율}^2 + \text{음성 비율}^2)
  $$

- **정보 이득(Information Gain)**:  
  부모 노드와 자식 노드 간의 불순도 차이.  
  결정 트리는 **정보 이득이 최대화**되도록 분할

- 사이킷런에서 `criterion='gini'` (기본) 또는 `'entropy'`로 변경 가능

#### ✂️ 가지치기 방법

- `max_depth`, `min_samples_split`, `min_impurity_decrease` 등의 매개변수로 트리 성장 제한 가능
- **결정 트리는 특성 간의 스케일 차이를 신경 쓰지 않음**  
  → **표준화 전처리 불필요**, 오히려 방해될 수 있음

### 3) 핵심 키워드 요약

- **결정 트리**: 예/아니오 기반 질문으로 분류, 설명력이 뛰어난 모델
- **불순도**: 노드 내 데이터의 혼합 정도를 수치화한 기준 (지니, 엔트로피)
- **정보 이득**: 분할로 인해 감소한 불순도의 크기
- **가지치기**: 과대적합 방지를 위한 트리 성장 제한
- **특성 중요도**: 각 특성이 전체 분류 정확도 향상에 기여한 정도

---

## 5-2. 교차 검증과 그리드 서치

### 1) 검증 세트(Validation Set)

- 테스트 세트는 오직 **최종 성능 평가용**으로 사용
- 훈련 중간에 테스트 세트를 자주 확인하면 **테스트 세트에 과적합될 위험**
- 해결책: 훈련 세트에서 일부를 **검증 세트**로 분리하여 하이퍼파라미터 튜닝 및 성능 평가

### 2) 교차 검증 (Cross Validation)

- 훈련 데이터 일부를 검증용으로 사용하면 훈련 데이터 양이 줄어드는 단점
- **교차 검증**은 훈련 세트를 여러 조각으로 나누고, 매번 다른 조각을 검증 세트로 사용하여 반복 평가
- **k-폴드 교차 검증**:
  - 데이터를 k등분하여 k번 반복
  - 각 반복마다 다른 폴드가 검증 세트 역할
  - 결과 평균을 사용하여 안정된 평가 수행

| 폴드 수 | 사용 예시 |
|--------|---------|
| 5      | 일반적인 기본 설정 |
| 10     | 더 안정적인 성능 평가 가능 |

- 사이킷런 함수: `cross_validate()`

#### 🔧 주의사항

- `cross_validate()`는 데이터를 **자동으로 섞지 않음**
- 분할기(splitter) 명시 필요:
  - **회귀**: `KFold`
  - **분류**: `StratifiedKFold` (클래스 비율 유지)

---

### 3) 하이퍼 파라미터 튜닝

#### 📌 용어 정리

| 구분             | 설명 |
|----------------|------|
| 모델 파라미터      | 학습 과정에서 자동으로 조정됨 (ex. 트리의 가중치) |
| 하이퍼 파라미터    | 사용자가 직접 지정해야 함 (ex. max_depth) |

- 다양한 하이퍼파라미터 조합을 실험하고 평가 → 성능 향상 도모

#### 🔍 그리드 서치 (Grid Search)

- 사이킷런 클래스: `GridSearchCV`
- **교차 검증 + 파라미터 조합 탐색**을 한 번에 수행
- `cv=5`가 기본 → 각 조합마다 5번 평가

예: `min_impurity_decrease` 값 5개 × 5-폴드 = **25개 모델 훈련**

- 병렬 처리: `n_jobs=-1` → 모든 CPU 코어 사용
- 결과 활용:
  - `best_estimator_`: 최고 성능 모델
  - `best_params_`: 최적 하이퍼파라미터 조합
  - `best_score_`: 최고 검증 점수

- `argmax()` 등으로 가장 높은 성능의 인덱스 찾기 가능

---

### 3-2) 랜덤 서치 (Random Search)

- 하이퍼파라미터의 범위가 넓거나 세밀한 조정이 어려울 경우 사용
- 지정한 **확률 분포에서 무작위 샘플링**하여 파라미터 설정

#### 📌 분포 함수 예시

- `randint(a, b)`:  
  → `[a, b)` 범위의 **정수**에서 무작위 추출 (이산형 분포)

- `uniform(a, b)`:
  → `[a, a + b]` 범위의 **실수(float)**에서 무작위 추출 (연속형 분포)

- 장점:  
  → 많은 조합을 시도하지 않고도 성능이 좋은 하이퍼파라미터 조합을 빠르게 찾을 수 있음

---

## ✅ 요약

- **결정 트리**는 직관적이고 해석 가능하지만 가지치기를 하지 않으면 과대적합되기 쉬움
- **불순도(지니, 엔트로피)**는 노드 분할 기준이며, 정보 이득은 성능 향상 측정 도구
- **교차 검증**은 모델의 일반화 성능을 안정적으로 평가
- **그리드 서치**는 가능한 모든 하이퍼파라미터 조합을 평가하여 최적 조합을 탐색
- **랜덤 서치**는 조합 수가 많을 때 효율적인 탐색 방법

## 5-3. 트리의 앙상블

### 1) 앙상블 학습이란?

- 여러 개의 모델을 결합하여 **단일 모델보다 더 뛰어난 예측 성능**을 도출하는 방법
- 일반적으로 **약한 학습기(weak learner)**를 결합하여 **강한 학습기(strong learner)**로 만드는 것이 목적
- 대표적인 앙상블 기법:
  - **배깅(Bagging)**: 여러 모델을 병렬로 학습하고 평균 또는 다수결로 예측
  - **부스팅(Boosting)**: 모델을 순차적으로 학습하며 오차를 보완

---

### 2) 정형 vs 비정형 데이터

| 구분         | 설명 |
|--------------|------|
| 정형 데이터   | 구조화된 표 형태의 데이터 (예: CSV, 테이블) |
| 비정형 데이터 | 이미지, 텍스트, 음성 등 비구조적 데이터 |

- **정형 데이터**에는 트리 기반 앙상블 모델이 높은 성능을 보임
- **비정형 데이터**는 신경망 기반 알고리즘이 효과적

---

### 3) 랜덤 포레스트 (Random Forest)

- 결정 트리를 여러 개 생성하여 앙상블하는 대표적인 **배깅(Bagging)** 기법
- 무작위성을 도입해 각 트리를 다양하게 구성하고, 결과를 종합하여 예측

#### 🔸 주요 개념

- **부트스트랩 샘플링 (Bootstrap Sampling)**  
  → 데이터셋에서 **중복 허용**하여 샘플을 생성

- **랜덤 특성 선택**  
  → 각 노드에서 일부 특성만 고려하여 최적의 분할 기준 선택

#### 🔸 예측 방식

- **분류 문제**:  
  각 트리의 클래스 확률 평균 → 가장 높은 확률의 클래스로 예측

- **회귀 문제**:  
  각 트리의 예측값을 단순 **평균**

#### 🔸 장점

- **과대적합 방지**: 무작위성 + 앙상블 → 일반화 성능 향상
- **OOB(Out-of-Bag) 평가 가능**:  
  부트스트랩 샘플에 포함되지 않은 데이터를 검증 세트처럼 사용 가능  
  → 별도의 교차 검증 없이도 성능 평가 가능

---

### 4) 엑스트라 트리 (Extra Trees)

- `Extremely Randomized Trees`의 줄임말
- 랜덤 포레스트와 유사하지만, 다음과 같은 차이점 존재:

| 항목                | 랜덤 포레스트                 | 엑스트라 트리                 |
|---------------------|------------------------------|-------------------------------|
| 샘플링 방식          | 부트스트랩 (중복 허용) 사용     | 전체 훈련 세트 사용             |
| 노드 분할 기준       | 최적 분할 선택                 | **완전 무작위 분할**           |
| 속도                | 보통                           | **더 빠름**                    |
| 과대적합 가능성      | 상대적으로 높을 수 있음        | **무작위성 덕분에 낮음**       |

- 회귀: `ExtraTreesRegressor`  
- 분류: `ExtraTreesClassifier`

---

### 5) 그레이디언트 부스팅 (Gradient Boosting)

- 이전 모델의 **오차를 보완**하는 방식으로 순차적으로 트리를 학습
- 각 트리는 **얕은 결정 트리**로 구성 (약한 학습기)

#### 🔸 특징

- 손실 함수를 줄이도록 **경사하강법(Gradient Descent)** 기반 최적화
- 각 트리는 앞선 트리의 예측 오류를 줄이기 위해 학습됨
- 예측값은 모든 트리의 결과를 누적하여 계산

#### 🔸 손실 함수

| 문제 유형 | 사용 손실 함수           |
|-----------|--------------------------|
| 분류      | 로지스틱 손실 함수       |
| 회귀      | 평균 제곱 오차 (MSE)     |

#### 🔸 장점

- **일반화 성능 우수**
- **과대적합에 강함**
- 특성 중요도 파악 가능

#### 🔸 단점

- 학습 속도가 느릴 수 있음
- 하이퍼파라미터 튜닝이 민감

---

### ✅ 요약

| 항목                 | 설명 |
|----------------------|------|
| **랜덤 포레스트**      | 여러 결정 트리를 무작위로 학습하여 다수결 or 평균 |
| **엑스트라 트리**      | 전체 데이터 사용, 무작위 분할로 속도 ↑ 과대적합 ↓ |
| **그레이디언트 부스팅** | 오차를 보완하며 순차적으로 트리 추가, 성능 ↑ |
| **OOB 평가**           | 랜덤 포레스트에서 부트스트랩 제외 데이터를 검증에 사용 |
| **정형 vs 비정형**     | 정형 → 트리 앙상블 / 비정형 → 신경망 모델 적합 |

---

